{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "attempt=1009\n",
    "mb = False\n",
    "\n",
    "num_channels_10 = 4\n",
    "num_channels_20 = 6\n",
    "\n",
=======
    "attempt=75\n",
    "\n",
    "num_channels = 4\n",
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
    "len_edge = 32 # of higher resolution image\n",
    "bs = 16\n",
    "\n",
    "h_dim = 20 * 20\n",
    "latent_dim = 15 * 15\n",
    "\n",
    "latvisdim = int(np.sqrt(latent_dim))\n",
    "tb_imgs_to_display = 4\n",
    "\n",
<<<<<<< HEAD
    "train_size = 160 * 5\n",
    "valid_size = bs\n",
=======
    "train_size = 320 # number of samples per scene\n",
    "valid_size = 320 / 5 # number of samples per scene\n",
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
    "\n",
    "recording_interval = 500\n",
    "epochs = 1000000\n",
    "\n",
    "testlogpath = './vae_logs/test/' + str(attempt)\n",
    "trainlogpath = './vae_logs/train/' + str(attempt)\n",
    "genlogpath = './vae_logs/gen/' + str(attempt)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
=======
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
   "outputs": [],
   "source": [
    "def normalization_parameters(fn):\n",
    "    from osgeo import gdal\n",
    "    S = gdal.Open(fn)\n",
    "    mns = []\n",
    "    sds = []\n",
    "    maxs = []\n",
    "    \n",
    "    for b in range(S.RasterCount):\n",
    "        B = S.GetRasterBand(b+1)\n",
    "        mn, sd = B.ComputeStatistics(1)[2:4]\n",
    "        mns.append(mn)\n",
    "        sds.append(sd)\n",
    "        maxs.append(B.GetMaximum())\n",
    "        \n",
    "    return([mns, sds, maxs])\n",
    "\n",
    "def sample_1s(filebase, batchsize, tilesize=128, normalize=False, flattened=False, fliprot=True):\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    alsos1 = False\n",
    "    \n",
    "    if type(filebase) == str:\n",
    "        multires = False\n",
    "        S2_10 = gdal.Open(filebase)\n",
    "        if normalize:\n",
    "            maxima_10 = np.array(normalization_parameters(filebase)[2])\n",
    "        \n",
    "    elif type(filebase) == list:#interpreted as multiple resolutions\n",
    "        if len(filebase) == 3:\n",
    "            alsos1 = True\n",
    "\n",
    "        tilesize = tilesize/2\n",
    "        multires = True\n",
    "        \n",
    "        if alsos1:\n",
    "            S1 = gdal.Open(filebase[0])\n",
    "            S2_10 = gdal.Open(filebase[1])\n",
    "            S2_20 = gdal.Open(filebase[2])\n",
    "            \n",
    "            if normalize:\n",
    "                maxima_S1 = np.array(normalization_parameters(filebase[0])[2])\n",
    "                maxima_10 = np.array(normalization_parameters(filebase[1])[2])\n",
    "                maxima_20 = np.array(normalization_parameters(filebase[2])[2])               \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            S2_10 = gdal.Open(filebase[0])\n",
    "            S2_20 = gdal.Open(filebase[1])\n",
    "            \n",
    "            if normalize:\n",
    "                maxima_10 = np.array(normalization_parameters(filebase[0])[2])\n",
    "                maxima_20 = np.array(normalization_parameters(filebase[1])[2])\n",
    "                \n",
    "    samples_S1 = []\n",
    "    samples_10 = []\n",
    "    \n",
    "    if multires:        \n",
    "        samples_20 = []\n",
    "        \n",
    "        if fliprot:\n",
    "            fac_for_aug = 8\n",
    "        else:\n",
    "            fac_for_aug = 1\n",
    "            \n",
    "        while len(samples_10) < (batchsize * fac_for_aug):\n",
    "            RX = np.random.randint(S2_20.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S2_20.RasterYSize-tilesize,size=1)\n",
    "            \n",
    "            A_10 = np.transpose(S2_10.ReadAsArray(RX[0] * 2, RY[0] * 2, tilesize * 2, tilesize * 2)).astype(np.float32)\n",
    "            A_20 = np.transpose(S2_20.ReadAsArray(RX[0], RY[0], tilesize, tilesize)).astype(np.float32)\n",
    "            S1unmasked = True\n",
    "            if alsos1:\n",
    "                A_S1 = np.transpose(S1.ReadAsArray(RX[0] * 2, RY[0] * 2, tilesize * 2, tilesize * 2)).astype(np.float32)\n",
    "                if np.min(A_S1) > 0:\n",
    "                    S1unmasked = True\n",
    "                else:\n",
    "                    S1unmasked = False\n",
    "            \n",
    "            if (np.min(A_10) > 0) & (np.min(A_20) > 0) & S1unmasked:\n",
    "                if normalize:\n",
    "                    A_10 = A_10 / maxima_10\n",
    "                    A_20 = A_20 / maxima_20\n",
    "                    if alsos1:\n",
    "                        A_S1 = A_S1 / maxima_S1\n",
    "                if flattened:\n",
    "                    A_10 = A_10.flatten()\n",
    "                    A_20 = A_20.flatten()\n",
    "                    if alsos1:\n",
    "                        A_S1.flatten()\n",
    "                if fliprot:\n",
    "                    for r in range(0,3):\n",
    "                        samples_10.append(np.rot90(A_10, r))\n",
    "                        samples_10.append(np.fliplr(np.rot90(A_10, r)))                        \n",
    "                        samples_20.append(np.rot90(A_20, r))\n",
    "                        samples_20.append(np.fliplr(np.rot90(A_20, r)))\n",
    "                        if alsos1:\n",
    "                            samples_S1.append(np.rot90(A_S1, r))\n",
    "                            samples_S1.append(np.fliplr(np.rot90(A_S1, r)))\n",
    "                else:\n",
    "                    samples_10.append(A_10)\n",
    "                    samples_20.append(A_20)\n",
    "                    if alsos1:\n",
    "                        samples_S1.append(A_S1)\n",
    "                \n",
    "        return([np.array(samples_10), np.array(samples_20), np.array(samples_S1)])\n",
    "                \n",
    "    else:\n",
    "        while len(samples_10) < batchsize:\n",
    "            RX = np.random.randint(S2_10.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S2_10.RasterYSize-tilesize,size=1)\n",
    "            \n",
    "            A_10 = np.transpose(S2_10.ReadAsArray(RX[0], RY[0], tilesize, tilesize).astype(np.float32))\n",
    "            \n",
    "            if np.min(A_10) > 0:            \n",
    "                if normalize:\n",
    "                    A_10 = A_10 / maxima_10\n",
    "                if flattened:\n",
    "                    A_10 = A_10.flatten()\n",
    "                    \n",
    "                samples_10.append(A_10)\n",
    "                \n",
    "        return(np.array(samples_10))\n",
    "    \n",
    "def sample_1s(filebase, batchsize, tilesize=128, normalize=False, flattened=False, fliprot=True):\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    alsos1 = False\n",
    "    \n",
    "    if type(filebase) == str:\n",
    "        multires = False\n",
    "        S2_10 = gdal.Open(filebase)\n",
    "        if normalize:\n",
    "            maxima_10 = np.array(normalization_parameters(filebase)[2])\n",
    "        \n",
    "    elif type(filebase) == list:#interpreted as multiple resolutions\n",
    "        if len(filebase) == 3:\n",
    "            alsos1 = True\n",
    "\n",
    "        tilesize = tilesize/2\n",
    "        multires = True\n",
    "        \n",
    "        if alsos1:\n",
    "            S1 = gdal.Open(filebase[0])\n",
    "            S2_10 = gdal.Open(filebase[1])\n",
    "            S2_20 = gdal.Open(filebase[2])\n",
    "            \n",
    "            if normalize:\n",
    "                maxima_S1 = np.array(normalization_parameters(filebase[0])[2])\n",
    "                maxima_10 = np.array(normalization_parameters(filebase[1])[2])\n",
    "                maxima_20 = np.array(normalization_parameters(filebase[2])[2])               \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            S2_10 = gdal.Open(filebase[0])\n",
    "            S2_20 = gdal.Open(filebase[1])\n",
    "            \n",
    "            if normalize:\n",
    "                maxima_10 = np.array(normalization_parameters(filebase[0])[2])\n",
    "                maxima_20 = np.array(normalization_parameters(filebase[1])[2])\n",
    "                \n",
    "    samples_S1 = []\n",
    "    samples_10 = []\n",
    "    \n",
    "    if multires:        \n",
    "        samples_20 = []\n",
    "        \n",
    "        if fliprot:\n",
    "            fac_for_aug = 8\n",
    "        else:\n",
    "            fac_for_aug = 1\n",
    "            \n",
    "        while len(samples_10) < (batchsize * fac_for_aug):\n",
    "            np.random.seed()\n",
    "            RX = np.random.randint(S2_20.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S2_20.RasterYSize-tilesize,size=1)\n",
    "            \n",
    "            A_10 = np.transpose(S2_10.ReadAsArray(RX[0] * 2, RY[0] * 2, tilesize * 2, tilesize * 2)).astype(np.float32)\n",
    "            A_20 = np.transpose(S2_20.ReadAsArray(RX[0], RY[0], tilesize, tilesize)).astype(np.float32)\n",
    "            S1unmasked = True\n",
    "            if alsos1:\n",
    "                A_S1 = np.transpose(S1.ReadAsArray(RX[0] * 2, RY[0] * 2, tilesize * 2, tilesize * 2)).astype(np.float32)\n",
    "                if np.min(A_S1) > 0:\n",
    "                    S1unmasked = True\n",
    "                else:\n",
    "                    S1unmasked = False\n",
    "            \n",
    "            if (np.min(A_10) > 0) & (np.min(A_20) > 0) & S1unmasked:\n",
    "                if normalize:\n",
    "                    A_10 = A_10 / maxima_10\n",
    "                    A_20 = A_20 / maxima_20\n",
    "                    if alsos1:\n",
    "                        A_S1 = A_S1 / maxima_S1\n",
    "                if flattened:\n",
    "                    A_10 = A_10.flatten()\n",
    "                    A_20 = A_20.flatten()\n",
    "                    if alsos1:\n",
    "                        A_S1.flatten()\n",
    "                if fliprot:\n",
    "                    for r in range(0,3):\n",
    "                        samples_10.append(np.rot90(A_10, r))\n",
    "                        samples_10.append(np.fliplr(np.rot90(A_10, r)))                        \n",
    "                        samples_20.append(np.rot90(A_20, r))\n",
    "                        samples_20.append(np.fliplr(np.rot90(A_20, r)))\n",
    "                        if alsos1:\n",
    "                            samples_S1.append(np.rot90(A_S1, r))\n",
    "                            samples_S1.append(np.fliplr(np.rot90(A_S1, r)))\n",
    "                else:\n",
    "                    samples_10.append(A_10)\n",
    "                    samples_20.append(A_20)\n",
    "                    if alsos1:\n",
    "                        samples_S1.append(A_S1)\n",
    "                \n",
    "        return([np.array(samples_10), np.array(samples_20), np.array(samples_S1)])\n",
    "                \n",
    "    else:\n",
    "        while len(samples_10) < batchsize:\n",
    "            RX = np.random.randint(S2_10.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S2_10.RasterYSize-tilesize,size=1)\n",
    "            \n",
    "            A_10 = np.transpose(S2_10.ReadAsArray(RX[0], RY[0], tilesize, tilesize).astype(np.float32))\n",
    "            \n",
    "            if np.min(A_10) > 0:            \n",
    "                if normalize:\n",
    "                    A_10 = A_10 / maxima_10\n",
    "                if flattened:\n",
    "                    A_10 = A_10.flatten()\n",
    "                    \n",
    "                samples_10.append(A_10)\n",
    "                \n",
    "        return(np.array(samples_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_name_dict = {}\n",
    "def layer_name(base_name):\n",
    "    if base_name not in layer_name_dict:\n",
    "        layer_name_dict[base_name] = 0\n",
    "    layer_name_dict[base_name] += 1\n",
    "    name = base_name + str(layer_name_dict[base_name])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolute(inp, name, kernel_size = 3, out_chans = 64, sz = 1):\n",
    "    inp_chans = inp.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        W = tf.get_variable('weights', [kernel_size, kernel_size, inp_chans, out_chans], initializer=tf.contrib.layers.xavier_initializer_conv2d(), regularizer=tf.contrib.layers.l2_regularizer(0.0005))#, name='weights')\n",
    "        b = tf.get_variable('biases', [out_chans], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "        conv = tf.nn.conv2d(inp, W, strides=[1, sz, sz, 1], padding='SAME')\n",
    "        conv = tf.contrib.layers.batch_norm(conv, scope=scope) # train?\n",
    "        conv = tf.nn.relu(conv+b)\n",
    "#        conv = tf.nn.dropout(conv, 0.8)\n",
    "    return conv\n",
    "\n",
    "def pooling(inp, name, factor=2):\n",
    "    pool = tf.nn.max_pool(inp, ksize=[1, factor, factor, 1], strides=[1, factor, factor, 1], padding='SAME', name=name)\n",
    "    return pool\n",
    "\n",
    "def convolute2(inp, name, kernel_size = 3, out_chans = 64, sz = 1):\n",
    "    inp_chans = inp.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(name, reuse=True) as scope:\n",
    "        W2 = tf.get_variable('weights', [kernel_size, kernel_size, inp_chans, out_chans], initializer=tf.contrib.layers.xavier_initializer_conv2d(), regularizer=tf.contrib.layers.l2_regularizer(0.0005))#, name='weights')\n",
    "        b2 = tf.get_variable('biases', [out_chans], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "        conv = tf.nn.conv2d(inp, W2, strides=[1, sz, sz, 1], padding='SAME')\n",
    "        conv = tf.contrib.layers.batch_norm(conv, scope=scope) # train?\n",
    "        conv = tf.nn.relu(conv+b2)\n",
    "#        conv = tf.nn.dropout(conv, 0.8)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "innest = 512\n",
    "inner = 256\n",
    "middle = 128\n",
    "outer = 64\n",
    "#outest = 16\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    X_10 = tf.placeholder(tf.float32, shape=([None, len_edge, len_edge, num_channels_10]))#bs\n",
    "    tf.summary.image('input_images', X_10[:, :, :, 0:3][:, :, :, ::-1], max_outputs=tb_imgs_to_display)\n",
    "    \n",
    "    if mb == True:\n",
    "        X_20 = tf.placeholder(tf.float32, shape=([None, len_edge / 2, len_edge / 2, num_channels_20]))\n",
    "        tf.summary.image('input_imagess', X_20[:, :, :, 0:3], max_outputs=tb_imgs_to_display)\n",
    "\n",
    "    #X_S1 = tf.placeholder(tf.float32, shape=([None, len_edge, len_edge, num_channels_S1]))\n",
    "        \n",
    "dw_h_convs = OrderedDict()\n",
    "up_h_convs = OrderedDict()\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#Build the network\n",
    "X_go_10 = X_10\n",
    "\n",
    "X_go_10 = convolute(X_go_10, layer_name('conv'), 3, outer, sz = 1)\n",
    "X_go_10 = convolute(X_go_10,layer_name('conv'),3,outer,sz = 1)\n",
    "dw_h_convs[1] = pooling(X_go_10, 'pool1')\n",
    "\n",
    "if mb == True:\n",
    "    X_go_20 = convolute(X_20, layer_name('conv20'),3,outer,sz = 1)\n",
    "    X_go_20 = convolute(X_go_20, layer_name('conv20'),3,outer,sz = 1)\n",
    "    dw_h_convs[1] = tf.concat([dw_h_convs[1], X_go_20], 3)\n",
    "\n",
    "dw_h_convs[1] = convolute(dw_h_convs[1],layer_name('conv'),3,middle)\n",
    "dw_h_convs[1] = convolute(dw_h_convs[1],layer_name('conv'),3,middle)\n",
    "dw_h_convs[2] = pooling(dw_h_convs[1], 'pool2')\n",
    "\n",
    "dw_h_convs[2] = convolute(dw_h_convs[2],layer_name('conv'),3,inner)\n",
    "dw_h_convs[2] = convolute(dw_h_convs[2],layer_name('conv'),3,inner)\n",
    "dw_h_convs[3] = pooling(dw_h_convs[2], 'pool3')\n",
    "\n",
    "dw_h_convs[3] = convolute(dw_h_convs[3],layer_name('conv'),3,innest)\n",
    "dw_h_convs[3] = convolute(dw_h_convs[3],layer_name('conv'),3,innest)\n",
    "dw_h_convs[4] = pooling(dw_h_convs[3], 'pool4')\n",
    "\n",
    "convdim = 2 * 2 * innest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = tf.reshape(dw_h_convs[4], [-1, convdim])\n",
    "\n",
    "W_enc = tf.get_variable('W_enc', [convdim, h_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_enc = tf.get_variable('b_enc', [h_dim], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "full1 = tf.nn.dropout(tf.nn.relu(tf.contrib.layers.batch_norm(tf.matmul(flattened, W_enc) + b_enc)), keep_prob=keep_prob)\n",
    "\n",
    "#W_mu = tf.get_variable('W_mu', [h_dim, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W_mu = tf.get_variable('W_mu', [h_dim, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_mu = tf.get_variable('b_mu', [latent_dim], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "mu = tf.matmul(full1, W_mu) + b_mu\n",
    "#mu = tf.matmul(flattened, W_mu) + b_mu\n",
    "\n",
    "\n",
    "W_logstd = tf.get_variable('W_logstd', [h_dim, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#W_logstd = tf.get_variable('W_logstd', [h_dim, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_logstd = tf.get_variable('b_logstd', [latent_dim], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "logstd = tf.matmul(full1, W_logstd) + b_logstd\n",
    "#logstd = tf.matmul(full1, W_logstd) + b_logstd\n",
    "\n",
    "noise = tf.random_normal([1, latent_dim])\n",
    "z = mu + tf.multiply(noise, tf.exp(.5*logstd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_visual = tf.reshape(z, [-1, latvisdim, latvisdim, 1])\n",
    "tf.summary.image('latents', z_visual, max_outputs=tb_imgs_to_display)\n",
    "\n",
    "W_dec = tf.get_variable('W_dec', [latent_dim, h_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_dec = tf.get_variable('b_dec', [h_dim], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "full2 = tf.nn.dropout(tf.nn.relu(tf.contrib.layers.batch_norm(tf.matmul(z, W_dec) + b_dec)), keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W_dec2 = tf.get_variable('W_dec2', [h_dim, convdim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#W_dec2 = tf.get_variable('W_dec2', [h_dim, 4 * 4 * inner], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_dec2 = tf.get_variable('b_dec2', [convdim], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "\n",
    "full3 = tf.nn.relu(tf.contrib.layers.batch_norm(tf.matmul(full2, W_dec2) + b_dec2))\n",
    "#full3 = tf.nn.relu(tf.contrib.layers.batch_norm(tf.matmul(z, W_dec2) + b_dec2))\n",
    "\n",
    "reshaped = tf.reshape(full3, [-1, 2, 2, innest])\n",
    "\n",
    "up_h_convs[0] = tf.image.resize_images(reshaped, [ reshaped.get_shape().as_list()[1]*2, \n",
    "                                                            reshaped.get_shape().as_list()[2]*2] )\n",
    "\n",
    "#up_h_convs[0] = tf.concat([up_h_convs[0], dw_h_convs[3] ],3 ) \n",
    "\n",
    "up_h_convs[0] = convolute(up_h_convs[0], layer_name('conv'), 3, innest)\n",
    "up_h_convs[0] = convolute(up_h_convs[0], layer_name('conv'), 3, innest)\n",
    "up_h_convs[1] = tf.image.resize_images(up_h_convs[0], [ up_h_convs[0].get_shape().as_list()[1]*2, \n",
    "                                                            up_h_convs[0].get_shape().as_list()[2]*2] ) \n",
    "\n",
    "#up_h_convs[1] = tf.concat([up_h_convs[1], dw_h_convs[2] ],3 ) \n",
    "up_h_convs[1] = convolute(up_h_convs[1], layer_name('conv'), 3, inner)\n",
    "up_h_convs[1] = convolute(up_h_convs[1], layer_name('conv'), 3, inner)\n",
    "up_h_convs[2] = tf.image.resize_images(up_h_convs[1], [ up_h_convs[1].get_shape().as_list()[1]*2, \n",
    "                                                            up_h_convs[1].get_shape().as_list()[2]*2] ) \n",
    "\n",
    "#up_h_convs[2] = tf.concat([up_h_convs[2], dw_h_convs[1] ],3 ) \n",
    "up_h_convs[2] = convolute(up_h_convs[2], layer_name('conv'), 3, middle)\n",
    "up_h_convs[2] = convolute(up_h_convs[2], layer_name('conv'), 3, middle)\n",
    "up_h_convs[3] = tf.image.resize_images(up_h_convs[2], [ up_h_convs[2].get_shape().as_list()[2]*2, \n",
    "                                                            up_h_convs[2].get_shape().as_list()[2]*2] )\n",
    "\n",
    "#up_h_convs[3] = tf.concat([up_h_convs[3], dw_h_convs[0] ],3 ) \n",
    "\n",
    "\n",
    "# 20 absplitten:\n",
    "#arf = tf.placeholder(tf.int32)\n",
    "\n",
    "if mb == True:\n",
    "    up_h_convs[3] = convolute(up_h_convs[3], layer_name('conv'), 3, outer * 2)\n",
    "    up_h_convs[3] = convolute(up_h_convs[3], layer_name('conv'), 3, outer * 2)\n",
    "    up_h_convs[3] = tf.slice(up_h_convs[3], [0, 0, 0, 0], [bs, len_edge / 2, len_edge / 2, outer])\n",
    "    lowres = tf.slice(up_h_convs[3], [0, 0, 0, outer], [bs, len_edge / 2, len_edge / 2, outer])\n",
    "    #conv and reconstruct lowres\n",
    "    lowres = convolute(lowres, layer_name('conv'), 3, outer)\n",
    "    #lowres = convolute(lowres, layer_name('conv'), 3, outer)\n",
    "    W_low = tf.get_variable('weights_low', [1, 1, outer, num_channels_20], initializer=tf.contrib.layers.xavier_initializer_conv2d(), regularizer=False)#, name='weights')\n",
    "    b_low = tf.get_variable('biases_low', [num_channels_20], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "    reconstruction_low = tf.nn.sigmoid(tf.nn.conv2d(lowres, W_low, strides=[1, 1, 1, 1], padding='SAME') + b_low)\n",
    "    \n",
    "up_h_convs[3] = convolute(up_h_convs[3], layer_name('conv'), 3, outer)\n",
    "#up_h_convs[3] = convolute(up_h_convs[3], layer_name('conv'), 3, outer)\n",
    "#up_h_convs[4] = tf.image.resize_images(up_h_convs[3], [ up_h_convs[3].get_shape().as_list()[2]*2, \n",
    "#                                                            up_h_convs[3].get_shape().as_list()[2]*2] )\n",
    "#up_h_convs[4] = convolute(up_h_convs[4], layer_name('conv'), 3, outer)\n",
    "#highres = convolute(highres, layer_name('conv'), 3, outer)\n",
    "W_rec = tf.get_variable('weights_rec', [1, 1, outer, 4], initializer=tf.contrib.layers.xavier_initializer_conv2d(), regularizer=False)#, name='weights')\n",
    "b_rec = tf.get_variable('biases_rec', [4], initializer=tf.constant_initializer(0.0), regularizer=None, dtype=tf.float32)\n",
    "reconstruction = tf.nn.sigmoid(tf.nn.conv2d(up_h_convs[3], W_rec, strides=[1, 1, 1, 1], padding='SAME') + b_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('reconst'):\n",
    "#    if num_channels_10 == 2:\n",
    "#        r_show = tf.concat([reconstruction, tf.expand_dims(reconstruction[:, :, :, 1], 3)], axis=3)\n",
    "#        tf.summary.image('reconstructed_images', r_show, max_outputs=tb_imgs_to_display)\n",
    "#    elif num_channels_10 > 3:\n",
    "    tf.summary.image('reconstructed_images', reconstruction[:, :, :, 0:3][:, :, :, ::-1], max_outputs=tb_imgs_to_display)\n",
    "    if mb == True:\n",
    "        tf.summary.image('reconstructed_imagess', reconstruction_low[:, :, :, 0:3], max_outputs=tb_imgs_to_display)\n",
    "#    else:\n",
    "#        tf.summary.image('reconstructed_images', reconstruction, max_outputs=tb_imgs_to_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = tf.contrib.layers.flatten(X_10)\n",
    "R_flat = tf.contrib.layers.flatten(reconstruction)\n",
    "log_likelihood = tf.reduce_sum(X_flat*tf.log(R_flat + 1e-9)+(1 - X_flat)*tf.log(1 - R_flat + 1e-9), reduction_indices=1)\n",
    "\n",
    "if mb == True:\n",
    "    #if multires_filters > 0:\n",
    "    X2_flat = tf.contrib.layers.flatten(X_20)\n",
    "    R2_flat = tf.contrib.layers.flatten(reconstruction_low)\n",
    "    log_likelihood2 = tf.reduce_sum(X2_flat*tf.log(R2_flat + 1e-9)+(1 - X2_flat)*tf.log(1 - R2_flat + 1e-9), reduction_indices=1)\n",
    "    log_likelihood = log_likelihood + log_likelihood2\n",
    "\n",
    "#log_likelihood = tf.reduce_sum(X_flat*tf.log(R_flat + 1e-9)+(1 - X_flat)*tf.log(1 - R_flat + 1e-9), reduction_indices=1)\n",
    "tf.summary.scalar('LogLike', tf.reduce_mean(log_likelihood))\n",
    "\n",
    "KL_term = -.5*tf.reduce_sum(1 + 2*logstd - tf.pow(mu,2) - tf.exp(2*logstd), reduction_indices=1)\n",
    "tf.summary.scalar('KL', tf.reduce_mean(KL_term))\n",
    "\n",
    "variational_lower_bound = tf.reduce_mean(log_likelihood - KL_term)\n",
    "tf.summary.scalar('cost', variational_lower_bound)\n",
    "\n",
    "validator = tf.cast(variational_lower_bound, tf.int32) # alibi\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(-variational_lower_bound)\n",
    "optimizer_likeli = tf.train.AdamOptimizer(1e-4).minimize(-log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 11,
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_z = tf.placeholder(tf.float32, shape=([None, latent_dim]))\n",
    "_up_h_convs = OrderedDict()\n",
    "\n",
    "#_full2 = tf.nn.dropout(tf.nn.relu(tf.contrib.layers.batch_norm(tf.matmul(_z, W_dec) + b_dec)), 0.5)\n",
    "_full3 = tf.nn.relu(tf.contrib.layers.batch_norm(tf.matmul(_z, W_dec2) + b_dec2))\n",
    "_reshaped = tf.reshape(_full3, [-1, 2, 2, innest])\n",
    "\n",
    "_up_h_convs[0] = tf.image.resize_images(_reshaped, [ _reshaped.get_shape().as_list()[1]*2, \n",
    "                                                            _reshaped.get_shape().as_list()[2]*2] )\n",
    "_up_h_convs[0] = convolute2(_up_h_convs[0], 'conv9', 3, innest)\n",
    "_up_h_convs[0] = convolute2(_up_h_convs[0], 'conv10', 3, innest)\n",
    "\n",
    "_up_h_convs[1] = tf.image.resize_images(_up_h_convs[0], [ _up_h_convs[0].get_shape().as_list()[1]*2, \n",
    "                                                            _up_h_convs[0].get_shape().as_list()[2]*2] ) \n",
    "\n",
    "_up_h_convs[1] = convolute2(_up_h_convs[1], 'conv11', 3, inner)\n",
    "_up_h_convs[1] = convolute2(_up_h_convs[1], 'conv12', 3, inner)\n",
    "_up_h_convs[2] = tf.image.resize_images(_up_h_convs[1], [ _up_h_convs[1].get_shape().as_list()[1]*2, \n",
    "                                                            _up_h_convs[1].get_shape().as_list()[2]*2] ) \n",
    "\n",
    "_up_h_convs[2] = convolute2(_up_h_convs[2], 'conv13', 3, middle)\n",
    "_up_h_convs[2] = convolute2(_up_h_convs[2], 'conv14', 3, middle)\n",
    "_up_h_convs[3] = tf.image.resize_images(_up_h_convs[2], [ _up_h_convs[2].get_shape().as_list()[2]*2, \n",
    "                                                            _up_h_convs[2].get_shape().as_list()[2]*2] ) \n",
    "_up_h_convs[3] = convolute2(_up_h_convs[3], 'conv15', 3, outer)\n",
    "_up_h_convs[3] = convolute2(_up_h_convs[3], 'conv16', 3, outer)\n",
    "\n",
    "_reconstruction = tf.nn.sigmoid(tf.nn.conv2d(_up_h_convs[3], W_rec, strides=[1, 1, 1, 1], padding='SAME') + b_rec)\n",
    "\n",
    "with tf.name_scope('reconstt'):\n",
    "    if num_channels == 2:\n",
    "        _r_show = tf.concat([_reconstruction, tf.expand_dims(_reconstruction[:, :, :, 1], 3)], axis=3)\n",
    "        tf.summary.image('gen_images', _r_show, max_outputs=tb_imgs_to_display)\n",
    "    elif num_channels > 3:\n",
    "        tf.summary.image('gen_images', _reconstruction[:, :, :, 0:3], max_outputs=tb_imgs_to_display)\n",
    "    else:\n",
    "        tf.summary.image('gen_images', _reconstruction, max_outputs=tb_imgs_to_display)\n",
    "        \n",
    "with tf.name_scope('reconstt'):\n",
    "    gensum = tf.summary.merge(['gen_images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "train_writer = tf.summary.FileWriter(trainlogpath)\n",
    "valid_writer = tf.summary.FileWriter(testlogpath)\n",
    "#gen_writer = tf.summary.FileWriter(genlogpath)\n",
    "train_writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def sample_some(n_per_scene=train_size, fliprot=True):\n",
    "    # S2 only\n",
    "    X_all_10 = []\n",
    "    X_all_20 = []\n",
    "    direc = '/run/media/ron/silver_small/conv_vae'\n",
    "    for s in ['A', 'B', 'C', 'D', 'E']:\n",
    "        names = glob.glob(direc + '/S2*' + s + '*tif')\n",
    "        names.sort()\n",
    "        print names\n",
    "        X_temp_10, X_temp_20, _ = sample_1s(names, n_per_scene, tilesize=len_edge, normalize=True, flattened=False, fliprot=fliprot)\n",
    "        X_all_10 = X_all_10 + list(X_temp_10)\n",
    "        X_all_20 = X_all_20 + list(X_temp_20)\n",
    "    X_all_10 = np.array(X_all_10)\n",
    "    X_all_20 = np.array(X_all_20)\n",
    "    np.random.seed(40)\n",
    "    np.random.shuffle(X_all_10)\n",
    "    np.random.seed(40)\n",
    "    np.random.shuffle(X_all_20)    \n",
    "    return [X_all_10, X_all_20]"
=======
    "X_all = sample_1s2(fn3, train_size, tilesize=len_edge, normalize=True, flattened=False)\n",
    "x_valid = sample_1s2(fn3, valid_size, tilesize=len_edge, normalize=True, flattened=False, fliprot=False)"
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
=======
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/run/media/ron/silver_small/conv_vae/S2_10_A.tif', '/run/media/ron/silver_small/conv_vae/S2_20_A.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_B.tif', '/run/media/ron/silver_small/conv_vae/S2_20_B.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_C.tif', '/run/media/ron/silver_small/conv_vae/S2_20_C.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_D.tif', '/run/media/ron/silver_small/conv_vae/S2_20_D.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_E.tif', '/run/media/ron/silver_small/conv_vae/S2_20_E.tif']\n"
     ]
    }
   ],
<<<<<<< HEAD
   "source": [
    "X_all_10, _ = sample_some(train_size, True)\n",
    "X_valid_10, _ = sample_some(valid_size, False)"
=======
   "source": [
    "# S2 only\n",
    "X_all_10 = []\n",
    "X_all_20 = []\n",
    "direc = '/run/media/ron/silver_small/conv_vae'\n",
    "for s in ['A', 'B', 'C', 'D', 'E']:\n",
    "    names = glob.glob(direc + '/S2*' + s + '*tif')\n",
    "    names.sort()\n",
    "    X_temp_10, X_temp_20 = sample_1s(names, train_size, tilesize=len_edge, normalize=True, flattened=False, fliprot=True)\n",
    "    X_all_10 = X_all_10 + list(X_temp_10)\n",
    "    X_all_20 = X_all_20 + list(X_temp_20)\n",
    "X_all_10 = np.array(X_all_10)\n",
    "X_all_20 = np.array(X_all_20)\n",
    "\n",
    "X_valid_10, X_valid_20 = sample_1s2(names, valid_size, tilesize=len_edge, normalize=True, flattened=False, fliprot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/run/media/ron/silver_small/conv_vae/S1_A.tif', '/run/media/ron/silver_small/conv_vae/S2_10_A.tif', '/run/media/ron/silver_small/conv_vae/S2_20_A.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S1_B.tif', '/run/media/ron/silver_small/conv_vae/S2_10_B.tif', '/run/media/ron/silver_small/conv_vae/S2_20_B.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S1_C.tif', '/run/media/ron/silver_small/conv_vae/S2_10_C.tif', '/run/media/ron/silver_small/conv_vae/S2_20_C.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S1_D.tif', '/run/media/ron/silver_small/conv_vae/S2_10_D.tif', '/run/media/ron/silver_small/conv_vae/S2_20_D.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S1_E.tif', '/run/media/ron/silver_small/conv_vae/S2_10_E.tif', '/run/media/ron/silver_small/conv_vae/S2_20_E.tif']\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# S1 and S2\n",
    "X_all_10 = []\n",
    "X_all_20 = []\n",
    "X_all_S1 = []\n",
    "direc = '/run/media/ron/silver_small/conv_vae'\n",
    "for s in ['A', 'B', 'C', 'D', 'E']:\n",
    "    names = glob.glob(direc + '/S*' + s + '*tif')\n",
    "    names.sort()\n",
    "    \n",
    "    X_temp_10, X_temp_20, X_temp_S1 = sample_1s(names, 10, tilesize=len_edge, normalize=True, flattened=False, fliprot=True)\n",
    "    X_all_10 = X_all_10 + list(X_temp_10)\n",
    "    X_all_20 = X_all_20 + list(X_temp_20)\n",
    "    X_all_S1 = X_all_S1 + list(X_temp_S1)\n",
    "X_all_10 = np.array(X_all_10)\n",
    "X_all_20 = np.array(X_all_20)\n",
    "X_all_S1 = np.array(X_all_S1)\n",
    "\n",
    "X_valid_10, X_valid_20, X_valid_S1 = sample_1s(names, 5, tilesize=len_edge, normalize=True, flattened=False, fliprot=False)"
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#X_all = sample_1s2(fn3, train_size*1000, tilesize=len_edge, normalize=True, flattened=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#, zed, reco, icks , z, reconstruction, X"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
=======
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
>>>>>>> 66daa75d4314a1b282ba438f576627063297b73e
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/run/media/ron/silver_small/conv_vae/S2_10_A.tif', '/run/media/ron/silver_small/conv_vae/S2_20_A.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_B.tif', '/run/media/ron/silver_small/conv_vae/S2_20_B.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_C.tif', '/run/media/ron/silver_small/conv_vae/S2_20_C.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_D.tif', '/run/media/ron/silver_small/conv_vae/S2_20_D.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_E.tif', '/run/media/ron/silver_small/conv_vae/S2_20_E.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_A.tif', '/run/media/ron/silver_small/conv_vae/S2_20_A.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_B.tif', '/run/media/ron/silver_small/conv_vae/S2_20_B.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_C.tif', '/run/media/ron/silver_small/conv_vae/S2_20_C.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_D.tif', '/run/media/ron/silver_small/conv_vae/S2_20_D.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_E.tif', '/run/media/ron/silver_small/conv_vae/S2_20_E.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_A.tif', '/run/media/ron/silver_small/conv_vae/S2_20_A.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_B.tif', '/run/media/ron/silver_small/conv_vae/S2_20_B.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_C.tif', '/run/media/ron/silver_small/conv_vae/S2_20_C.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_D.tif', '/run/media/ron/silver_small/conv_vae/S2_20_D.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_E.tif', '/run/media/ron/silver_small/conv_vae/S2_20_E.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_A.tif', '/run/media/ron/silver_small/conv_vae/S2_20_A.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_B.tif', '/run/media/ron/silver_small/conv_vae/S2_20_B.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_C.tif', '/run/media/ron/silver_small/conv_vae/S2_20_C.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_D.tif', '/run/media/ron/silver_small/conv_vae/S2_20_D.tif']\n",
      "['/run/media/ron/silver_small/conv_vae/S2_10_E.tif', '/run/media/ron/silver_small/conv_vae/S2_20_E.tif']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-253c1611ed18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_10\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_20\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_10\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mrecording_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = bs\n",
    "ep = 0\n",
    "for ep in range(epochs):\n",
    "#    print ep\n",
    "    x_batch = X_all_10[step-bs:step]\n",
    "#    x_batch_20 = X_all_20[step-bs:step]\n",
    "    \n",
    "    if mb == True:\n",
    "        _, s = sess.run([optimizer, merged_summary], feed_dict={X_10: x_batch, X_20: x_batch_20, keep_prob: 0.8})\n",
    "    else:\n",
    "        _, s = sess.run([optimizer, merged_summary], feed_dict={X_10: x_batch, keep_prob: 0.8})\n",
    "    \n",
    "    if (ep%recording_interval == 0):\n",
    "        train_writer.add_summary(s, ep)\n",
    "        if mb == True:\n",
    "            vvv, s = sess.run([validator, merged_summary], feed_dict={X_10: X_valid_10[:bs], X_20: X_valid_20[:bs], keep_prob:1.})\n",
    "        else:\n",
    "            vvv, s = sess.run([validator, merged_summary], feed_dict={X_10: X_valid_10, keep_prob:1.})\n",
    "            \n",
    "        valid_writer.add_summary(s, ep)        \n",
    "#        z_mu = np.random.normal(size=[bs, latent_dim])\n",
    "#        s = sess.run([gensum], feed_dict={_z: z_mu})\n",
    "#        gen_writer.add_summary(s, ep)\n",
    "    if (ep%10000 == 0) & (ep != 0):\n",
    "        X_all_10, _ = sample_some(train_size, True)\n",
    "        \n",
    "    step += bs\n",
    "\n",
    "    if step == train_size:\n",
    "        step = bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi = sess.run([_reconstruction], feed_dict={_z: z_mu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_mu = np.random.normal(size=[bs, latent_dim])\n",
    "hi = sess.run([_reconstruction], feed_dict={_z: z_mu})\n",
    "#train_writer.add_summary(s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ho = sess.run([_reconstruction], feed_dict={_z: zed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(hi[0][9, :, :, ::-1][:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave\n",
    "imsave('blabla2.tif', hi[0][1][:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.fromarray(hi[0][0][:,:,:3])\n",
    "im.save(\"your_file2.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_batch[5, :, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(ho[0][5, :, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, s, zed, reco, icks = sess.run([optimizer, merged_summary, z, reconstruction, X], feed_dict={X: x_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "icks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(icks[0, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(reco[0, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(ho[0][0, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ho = sess.run([_reconstruction], feed_dict={_z: zed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S1 and S2\n",
    "X_all_10 = []\n",
    "X_all_20 = []\n",
    "X_all_S1 = []\n",
    "direc = '/run/media/ron/silver_small/conv_vae'\n",
    "for s in ['A', 'B', 'C', 'D', 'E']:\n",
    "    names = glob.glob(direc + '/S*' + s + '*tif')\n",
    "    names.sort()\n",
    "    \n",
    "    X_temp_10, X_temp_20, X_temp_S1 = sample_1s(names, 10, tilesize=len_edge, normalize=True, flattened=False, fliprot=True)\n",
    "    X_all_10 = X_all_10 + list(X_temp_10)\n",
    "    X_all_20 = X_all_20 + list(X_temp_20)\n",
    "    X_all_S1 = X_all_S1 + list(X_temp_S1)\n",
    "X_all_10 = np.array(X_all_10)\n",
    "X_all_20 = np.array(X_all_20)\n",
    "X_all_S1 = np.array(X_all_S1)\n",
    "\n",
    "X_valid_10, X_valid_20, X_valid_S1 = sample_1s(names, 5, tilesize=len_edge, normalize=True, flattened=False, fliprot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
