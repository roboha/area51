{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #matrix math\n",
    "import math\n",
    "import tensorflow as tf #machine learning\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_channels = 1\n",
    "len_edge = 28\n",
    "bs = 32\n",
    "n_filters = [1, 16, 16]\n",
    "h_dim = 133\n",
    "latent_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "    \n",
    "def fc_layer(inp, channels_in, channels_out, name='fc'):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.zeros([channels_in, channels_out]), name='W')\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "        return tf.nn.relu(tf.matmul(inp, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reconstructed_images:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use TensorBoard to visualize: this code fails to create meaningful latent variables\n",
    "#(within a batch, each looks the same), and therefore also fails at reconstruction.\n",
    "#Both log-likelihood error and KL divergence appear to develop \"normally\", yet KL drops to 0 very quickly?\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(tf.float32, shape=([None, len_edge, len_edge, num_channels]))#bs\n",
    "    tf.summary.image('input_images', X, max_outputs=5)\n",
    "\n",
    "cur_input = X\n",
    "\n",
    "Ws = []    \n",
    "shapes = []\n",
    "\n",
    "for l, n_out in enumerate(n_filters[1:]):\n",
    "    cur_name = 'conv' + str(l)    \n",
    "    n_input = cur_input.get_shape().as_list()[3]\n",
    "    shapes.append(cur_input.get_shape().as_list())\n",
    "    with tf.name_scope('conv_indecon_' + str(l)):\n",
    "        W = tf.Variable(tf.random_uniform([3, 3, n_input, n_out], -1.0/math.sqrt(n_input), 1.0/math.sqrt(n_input)), name='weights')\n",
    "        b = tf.Variable(tf.zeros([n_out]), name='bias')\n",
    "        Ws.append(W)   \n",
    "        conv = tf.nn.conv2d(cur_input, W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        act = tf.nn.sigmoid(conv+b)#tf.nn.relu(conv,b)    \n",
    "    cur_input = act\n",
    "\n",
    "with tf.name_scope('Dense'):\n",
    "    with tf.name_scope('Fully_Encode'):\n",
    "        flattened = tf.reshape(cur_input, [-1, 7 * 7 * n_filters[-1]])# ...\n",
    "        full1 = fc_layer(flattened, 7 * 7 * n_filters[-1], h_dim, 'fc1')\n",
    "    with tf.name_scope('Mu'):\n",
    "        W_mu = weight_variable([h_dim, latent_dim], 'W_mu')\n",
    "        b_mu = bias_variable([latent_dim], 'b_mu')\n",
    "        mu = tf.matmul(full1, W_mu) + b_mu\n",
    "    with tf.name_scope('Logstd'):\n",
    "        W_logstd = weight_variable([h_dim, latent_dim], 'W_logstd')\n",
    "        b_logstd = bias_variable([latent_dim], 'b_logstd')\n",
    "        logstd = tf.matmul(full1, W_logstd) + b_logstd\n",
    "    with tf.name_scope('VAE_final'):\n",
    "        noise = tf.random_normal([1, latent_dim])\n",
    "        z = mu + tf.multiply(noise, tf.exp(.5*logstd))\n",
    "    with tf.name_scope('Fully_Decode'):\n",
    "        full2 = fc_layer(z, latent_dim, h_dim, 'fc2')\n",
    "        full3 = fc_layer(full2, h_dim, 7 * 7 * n_filters[-1], 'fc3')\n",
    "    reshaped = tf.reshape(full3, [-1, 7, 7, n_filters[-1]])\n",
    "        \n",
    "z_visual = tf.reshape(z, [-1, 4, 4, 1])\n",
    "tf.summary.image('latents', z_visual, max_outputs=5)\n",
    "tf.summary.histogram('Latent', z)\n",
    "\n",
    "Ws.reverse()\n",
    "shapes.reverse()\n",
    "cur_input = reshaped\n",
    "\n",
    "for l, shape in enumerate(shapes):\n",
    "    cur_name = 'deconv' + str(l)  \n",
    "    W = Ws[l]    \n",
    "    with tf.name_scope('conv_indecon_' + str(len(Ws)-(l+1))):\n",
    "        b = tf.Variable(tf.zeros([W.get_shape().as_list()[2]]), name='bias_dec_'+str(l))\n",
    "        dec = tf.nn.conv2d_transpose(cur_input, W, tf.stack([tf.shape(X)[0], shape[1], shape[2], shape[3]]), strides=[1,2,2,1], padding='SAME')\n",
    "        if l+1 < len(shapes):\n",
    "            act = tf.nn.sigmoid(dec+b)\n",
    "            cur_input = act\n",
    "\n",
    "with tf.name_scope('reconst'):\n",
    "    reconstruction = tf.nn.sigmoid(dec + b)\n",
    "\n",
    "tf.summary.image('reconstructed_images', reconstruction, max_outputs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood = tf.reduce_sum(X*tf.log(reconstruction + 1e-9)+(1 - X)*tf.log(1 - reconstruction + 1e-9), reduction_indices=1)\n",
    "tf.summary.scalar('LogLike', tf.reduce_mean(log_likelihood))\n",
    "\n",
    "KL_term = -.5*tf.reduce_sum(1 + 2*logstd - tf.pow(mu,2) - tf.exp(2*logstd), reduction_indices=1)\n",
    "tf.summary.scalar('KL', tf.reduce_mean(KL_term))\n",
    "\n",
    "variational_lower_bound = tf.reduce_mean(log_likelihood - KL_term)\n",
    "tf.summary.scalar('cost', variational_lower_bound)\n",
    "\n",
    "#optimizer = tf.train.AdadeltaOptimizer().minimize(-variational_lower_bound)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(-variational_lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter('./vae_logs/2')\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: -22.8667087555\n",
      "Iteration: 10, Loss: -22.5926856995\n",
      "Iteration: 20, Loss: -22.4780864716\n",
      "Iteration: 30, Loss: -22.1377544403\n",
      "Iteration: 40, Loss: -22.0238437653\n",
      "Iteration: 50, Loss: -21.8365535736\n",
      "Iteration: 60, Loss: -21.6610164642\n",
      "Iteration: 70, Loss: -21.4929962158\n",
      "Iteration: 80, Loss: -21.2866859436\n",
      "Iteration: 90, Loss: -21.0990543365\n",
      "Iteration: 100, Loss: -20.9184150696\n",
      "Iteration: 110, Loss: -20.7674388885\n",
      "Iteration: 120, Loss: -20.5729026794\n",
      "Iteration: 130, Loss: -20.3891487122\n",
      "Iteration: 140, Loss: -20.2836494446\n",
      "Iteration: 150, Loss: -20.0570163727\n",
      "Iteration: 160, Loss: -19.878156662\n",
      "Iteration: 170, Loss: -19.8642692566\n",
      "Iteration: 180, Loss: -19.6327972412\n",
      "Iteration: 190, Loss: -19.5402317047\n",
      "Iteration: 200, Loss: -19.3276462555\n",
      "Iteration: 210, Loss: -19.1888599396\n",
      "Iteration: 220, Loss: -19.0823688507\n",
      "Iteration: 230, Loss: -18.9490509033\n",
      "Iteration: 240, Loss: -18.807806015\n",
      "Iteration: 250, Loss: -18.7744865417\n",
      "Iteration: 260, Loss: -18.6289558411\n",
      "Iteration: 270, Loss: -18.341337204\n",
      "Iteration: 280, Loss: -18.1911296844\n",
      "Iteration: 290, Loss: -18.2777767181\n",
      "Iteration: 300, Loss: -17.9757270813\n",
      "Iteration: 310, Loss: -17.8477115631\n",
      "Iteration: 320, Loss: -17.7167263031\n",
      "Iteration: 330, Loss: -17.6524181366\n",
      "Iteration: 340, Loss: -17.7055988312\n",
      "Iteration: 350, Loss: -17.4492149353\n",
      "Iteration: 360, Loss: -17.2834358215\n",
      "Iteration: 370, Loss: -17.1258049011\n",
      "Iteration: 380, Loss: -17.2994709015\n",
      "Iteration: 390, Loss: -16.9888076782\n",
      "Iteration: 400, Loss: -16.6731777191\n",
      "Iteration: 410, Loss: -16.9323177338\n",
      "Iteration: 420, Loss: -16.5327739716\n",
      "Iteration: 430, Loss: -16.5660457611\n",
      "Iteration: 440, Loss: -16.2805919647\n",
      "Iteration: 450, Loss: -16.5968093872\n",
      "Iteration: 460, Loss: -16.4753475189\n",
      "Iteration: 470, Loss: -16.1729412079\n",
      "Iteration: 480, Loss: -16.2611675262\n",
      "Iteration: 490, Loss: -16.0892448425\n",
      "Iteration: 500, Loss: -15.9360485077\n",
      "Iteration: 510, Loss: -15.7463998795\n",
      "Iteration: 520, Loss: -15.5742349625\n",
      "Iteration: 530, Loss: -16.1041965485\n",
      "Iteration: 540, Loss: -15.701634407\n",
      "Iteration: 550, Loss: -15.7066316605\n",
      "Iteration: 560, Loss: -15.8095006943\n",
      "Iteration: 570, Loss: -15.5496463776\n",
      "Iteration: 580, Loss: -15.3503847122\n",
      "Iteration: 590, Loss: -15.2226390839\n",
      "Iteration: 600, Loss: -15.2477207184\n",
      "Iteration: 610, Loss: -15.1789484024\n",
      "Iteration: 620, Loss: -15.2588939667\n",
      "Iteration: 630, Loss: -15.1145191193\n",
      "Iteration: 640, Loss: -15.3756084442\n",
      "Iteration: 650, Loss: -14.3628454208\n",
      "Iteration: 660, Loss: -14.8774938583\n",
      "Iteration: 670, Loss: -14.4540195465\n",
      "Iteration: 680, Loss: -15.0730838776\n",
      "Iteration: 690, Loss: -14.8800907135\n",
      "Iteration: 700, Loss: -14.9465036392\n",
      "Iteration: 710, Loss: -14.235411644\n",
      "Iteration: 720, Loss: -14.6906366348\n",
      "Iteration: 730, Loss: -14.820306778\n",
      "Iteration: 740, Loss: -14.4548416138\n",
      "Iteration: 750, Loss: -14.6270341873\n",
      "Iteration: 760, Loss: -14.7374744415\n",
      "Iteration: 770, Loss: -13.8943920135\n",
      "Iteration: 780, Loss: -13.5595064163\n",
      "Iteration: 790, Loss: -13.7336454391\n",
      "Iteration: 800, Loss: -13.1820268631\n",
      "Iteration: 810, Loss: -13.5985059738\n",
      "Iteration: 820, Loss: -13.8740987778\n",
      "Iteration: 830, Loss: -13.5480775833\n",
      "Iteration: 840, Loss: -13.5503616333\n",
      "Iteration: 850, Loss: -13.7023906708\n",
      "Iteration: 860, Loss: -13.6627855301\n",
      "Iteration: 870, Loss: -13.5838260651\n",
      "Iteration: 880, Loss: -13.6251621246\n",
      "Iteration: 890, Loss: -13.3159227371\n",
      "Iteration: 900, Loss: -14.0775527954\n",
      "Iteration: 910, Loss: -13.7367601395\n",
      "Iteration: 920, Loss: -13.5471401215\n",
      "Iteration: 930, Loss: -13.6463537216\n",
      "Iteration: 940, Loss: -13.3130722046\n",
      "Iteration: 950, Loss: -13.7401027679\n",
      "Iteration: 960, Loss: -13.1117238998\n",
      "Iteration: 970, Loss: -12.2154769897\n",
      "Iteration: 980, Loss: -12.8822050095\n",
      "Iteration: 990, Loss: -14.0795125961\n",
      "Iteration: 1000, Loss: -12.6811666489\n",
      "Iteration: 1010, Loss: -13.3200235367\n",
      "Iteration: 1020, Loss: -12.6162414551\n",
      "Iteration: 1030, Loss: -12.8129024506\n",
      "Iteration: 1040, Loss: -12.9224948883\n",
      "Iteration: 1050, Loss: -13.5182619095\n",
      "Iteration: 1060, Loss: -13.0954532623\n",
      "Iteration: 1070, Loss: -12.9934844971\n",
      "Iteration: 1080, Loss: -13.1681203842\n",
      "Iteration: 1090, Loss: -12.458193779\n",
      "Iteration: 1100, Loss: -12.8361644745\n",
      "Iteration: 1110, Loss: -13.2428007126\n",
      "Iteration: 1120, Loss: -12.8116140366\n",
      "Iteration: 1130, Loss: -13.103676796\n",
      "Iteration: 1140, Loss: -12.5852994919\n",
      "Iteration: 1150, Loss: -11.7294149399\n",
      "Iteration: 1160, Loss: -12.8884048462\n",
      "Iteration: 1170, Loss: -12.3240356445\n",
      "Iteration: 1180, Loss: -12.8139057159\n",
      "Iteration: 1190, Loss: -12.0857572556\n",
      "Iteration: 1200, Loss: -12.5087099075\n",
      "Iteration: 1210, Loss: -12.7483778\n",
      "Iteration: 1220, Loss: -12.8960676193\n",
      "Iteration: 1230, Loss: -12.121096611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d869e0ea369d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#    x_batch = sample_a_batch(filename, bs, 64, sb=2, normalize=stats, flattened=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mrecording_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 1000000\n",
    "recording_interval = 10\n",
    "variational_lower_bound_array = []\n",
    "log_likelihood_array = []\n",
    "KL_term_array = []\n",
    "iteration_array = [i*recording_interval for i in range(num_iterations/recording_interval)]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    mn_l = mnist.train.next_batch(bs)[0]\n",
    "    x_batch = np.reshape(mn_l, [bs, 28, 28, 1])\n",
    "#    x_batch = sample_a_batch(filename, bs, 64, sb=2, normalize=stats, flattened=False)\n",
    "    _, s = sess.run([optimizer, merged_summary], feed_dict={X: x_batch})\n",
    "    writer.add_summary(s, i)\n",
    "    if (i%recording_interval == 0):\n",
    "        #every 1K iterations record these values\n",
    "        vlb_eval = variational_lower_bound.eval(feed_dict={X: x_batch})\n",
    "        print \"Iteration: {}, Loss: {}\".format(i, vlb_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEXT CELLS TO BE IGNORED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filename = '/run/media/ron/silver_small/twelve_months/3d/S1A_IW_GRDH_1SDV_20160325T083601_20160325T083630_010523_00FA23_6F51.tif'\n",
    "#stats = normalization_parameters(filename)\n",
    "\n",
    "def conv_layer(inp, channels_in, channels_out, vscope, name='conv'):\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scopec(vscope):\n",
    "            tf.Variable(tf.zeros([3, 3, channels_in, channels_out]), name='W')\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "        conv = tf.nn.conv2d(inp, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('activations', act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "def conv_t_layer(inp, channels_in, channels_out, name='conv_t'):#, activation=tf.nn.relu):\n",
    "    with tf.name_scope(name):\n",
    "        #w = tf.Variable(tf.zeros([3, 3, channels_out, channels_in]), name='W')\n",
    "        w = conv1\n",
    "        b = tf.Variable(tf.zeros([conv1.get_shape().as_list()[2]]))\n",
    "        \n",
    "        tf.nn.conv2d_transpose(inp, W, tf.stack([]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        b = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "        batch_size = 32 #tf.shape(inp)[0]\n",
    "        deconv_shape = tf.stack([batch_size, inp.shape[2].value * 2, inp.shape[2].value * 2, channels_out])\n",
    "        conv_t = tf.nn.conv2d_transpose(inp, w, deconv_shape, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #act = activation(conv_t + b)\n",
    "        act = tf.nn.relu(conv_t + b)\n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('activations', act)\n",
    "        return act#tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "def sample_a_batch(filebase, batchsize, tilesize=128, sb=0, normalize=False, flattened=True):\n",
    "    # can sample for spatio-temporal (single_file = False), and spatial-only case (single_file = True).\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    if '.' in filebase:\n",
    "        single_file = True\n",
    "        S = gdal.Open(filebase)\n",
    "    else:\n",
    "        single_file = False\n",
    "        S = gdal.Open(filebase + '_1.vrt')\n",
    "        \n",
    "    samples = []\n",
    "    \n",
    "    if single_file:\n",
    "        while len(samples) < batchsize:\n",
    "            RX = np.random.randint(S.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S.RasterYSize-tilesize,size=1)\n",
    "            \n",
    "            #A = np.transpose(S.ReadAsArray(RX[0], RY[0], tilesize, tilesize))           \n",
    "            \n",
    "            if sb:\n",
    "                B = S.GetRasterBand(sb)\n",
    "                A = np.transpose(B.ReadAsArray(RX[0], RY[0], tilesize, tilesize))                \n",
    "                #print np.min(A)\n",
    "                if np.min(A) > 0:\n",
    "                    if normalize:\n",
    "                        A = A / normalize[2][sb-1]                        \n",
    "                    A = np.expand_dims(A, 2)                    \n",
    "                    if flattened:\n",
    "                        A = A.flatten()                        \n",
    "                    samples.append(A)\n",
    "            else:\n",
    "                A = np.transpose(S.ReadAsArray(RX[0], RY[0], tilesize, tilesize))\n",
    "                if np.min(A) > 0:\n",
    "                    samples.append(A)\n",
    "        \n",
    "    else: # must be overhauled\n",
    "        while len(samples) < batchsize:\n",
    "            RX = np.random.randint(S.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S.RasterYSize-tilesize,size=1) \n",
    "            \n",
    "            skip_loc = False\n",
    "            months = []\n",
    "            \n",
    "            for m in range(1,13):\n",
    "                S = gdal.Open(filebase + '_' + str(m) + '.vrt')\n",
    "                A = np.transpose(S.ReadAsArray(RX[i], RY[i], tilesize, tilesize))\n",
    "                if np.min(A) == 0.0:\n",
    "                    skip_loc = True\n",
    "                    break\n",
    "                else:\n",
    "                    months.append(A)                \n",
    "            if not skip_loc:\n",
    "                months = np.array(months)\n",
    "                samples.append(months)\n",
    "        \n",
    "    return np.array(samples)\n",
    "\n",
    "def normalization_parameters(fn):\n",
    "    from osgeo import gdal\n",
    "    S = gdal.Open(fn)\n",
    "    mns = []\n",
    "    sds = []\n",
    "    maxs = []\n",
    "    \n",
    "    for b in range(S.RasterCount):\n",
    "        B = S.GetRasterBand(b+1)\n",
    "        mn, sd = B.ComputeStatistics(1)[2:4]\n",
    "        mns.append(mn)\n",
    "        sds.append(sd)\n",
    "        maxs.append(B.GetMaximum())\n",
    "        \n",
    "    return mns, sds, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(tf.float32, shape=([None, len_edge, len_edge, num_channels]))#bs\n",
    "    tf.summary.image('input_images', X, max_outputs=3)\n",
    "\n",
    "conv1 = conv_layer(X, num_channels, filters[0], 'conv1')\n",
    "conv2 = conv_layer(conv1, conv1.shape[-1].value, filters[1], 'conv2')\n",
    "conv3 = conv_layer(conv2, conv2.shape[-1].value, filters[2], 'conv3')\n",
    "conv4 = conv_layer(conv3, conv3.shape[-1].value, filters[3], 'conv4')\n",
    "\n",
    "#with tf.name_scope('Dense_Encode'):\n",
    "flattened = tf.reshape(conv4, [-1, 2 * 2 * filters[-1]])\n",
    "full1 = fc_layer(flattened, 2 * 2 * filters[-1], h_dim, 'fc1') # channels_in ?????\n",
    "\n",
    "W_mu = weight_variable([h_dim, latent_dim], 'W_mu')\n",
    "b_mu = bias_variable([latent_dim], 'b_mu')\n",
    "mu = FC_layer(full1, W_mu, b_mu)\n",
    "W_logstd = weight_variable([h_dim, latent_dim], 'W_logstd')\n",
    "b_logstd = bias_variable([latent_dim], 'b_logstd')\n",
    "logstd = FC_layer(full1, W_logstd, b_logstd)\n",
    "\n",
    "noise = tf.random_normal([1, latent_dim])\n",
    "z = mu + tf.multiply(noise, tf.exp(.5*logstd))\n",
    "\n",
    "z_visual = tf.reshape(z, [-1, 6, 6, 1])\n",
    "tf.summary.image('latents', z_visual, max_outputs=3)\n",
    "\n",
    "tf.summary.histogram('Latent', z)\n",
    "\n",
    "full2 = fc_layer(z, latent_dim, h_dim, 'fc2')\n",
    "full3 = fc_layer(full2, h_dim, 2 * 2 * filters[-1], 'fc3')# ???????\n",
    "\n",
    "reshaped = tf.reshape(full3, [-1, 2, 2, filters[-1]])\n",
    "\n",
    "conv_t1 = conv_t_layer(reshaped, filters[1], filters[1], 'conv_t1')\n",
    "conv_t2 = conv_t_layer(conv_t1, filters[1], filters[1], 'conv_t2')\n",
    "conv_t3 = conv_t_layer(conv_t2, filters[1], filters[1], 'conv_t3')\n",
    "reconstruction = conv_t_layer(conv_t3, filters[1], 1, 'conv_t4')#, tf.nn.sigmoid)\n",
    "\n",
    "tf.summary.image('reconstructed_images', reconstruction, max_outputs=3) #............"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
