{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #matrix math\n",
    "import math\n",
    "import tensorflow as tf #machine learning\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named datahelpers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6da834d2e73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatahelpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named datahelpers"
     ]
    }
   ],
   "source": [
    "import datahelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_channels = 1\n",
    "len_edge = 28\n",
    "bs = 16\n",
    "#n_filters = [1, 16, 16, 16, 16, 16]\n",
    "n_filters = [num_channels, 16, 16] # for mnist\n",
    "h_dim = 8\n",
    "latent_dim = 4\n",
    "inedge = len_edge / (2 ** (len(n_filters)-1))\n",
    "latvisdim = int(np.sqrt(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_a_batch(filebase, batchsize, tilesize=128, sb=0, normalize=False, flattened=True):\n",
    "    # can sample for spatio-temporal (single_file = False), and spatial-only case (single_file = True).\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    if '.' in filebase:\n",
    "        single_file = True\n",
    "        S = gdal.Open(filebase)\n",
    "    else:\n",
    "        single_file = False\n",
    "        S = gdal.Open(filebase + '_1.vrt')\n",
    "        \n",
    "    samples = []\n",
    "    \n",
    "    if single_file:\n",
    "        while len(samples) < batchsize:\n",
    "            RX = np.random.randint(S.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S.RasterYSize-tilesize,size=1)\n",
    "            \n",
    "            #A = np.transpose(S.ReadAsArray(RX[0], RY[0], tilesize, tilesize))           \n",
    "            \n",
    "            if sb:\n",
    "                B = S.GetRasterBand(sb)\n",
    "                A = np.transpose(B.ReadAsArray(RX[0], RY[0], tilesize, tilesize))                \n",
    "                #print np.min(A)\n",
    "                if np.min(A) > 0:\n",
    "                    if normalize:\n",
    "                        A = A / normalize[2][sb-1]                        \n",
    "                    A = np.expand_dims(A, 2)                    \n",
    "                    if flattened:\n",
    "                        A = A.flatten()                        \n",
    "                    samples.append(A)\n",
    "            else:\n",
    "                A = np.transpose(S.ReadAsArray(RX[0], RY[0], tilesize, tilesize))\n",
    "                if np.min(A) > 0:\n",
    "                    if normalize:\n",
    "                        for b in range(A.shape[3]):\n",
    "                            A[:, :, b] = A[:, :, b] / normalize[2][b]\n",
    "                        if flattened:\n",
    "                            A = A.flatten()                            \n",
    "                        samples.append(A)\n",
    "        \n",
    "    else: # must be overhauled\n",
    "        while len(samples) < batchsize:\n",
    "            RX = np.random.randint(S.RasterXSize-tilesize,size=1)\n",
    "            RY = np.random.randint(S.RasterYSize-tilesize,size=1) \n",
    "            \n",
    "            skip_loc = False\n",
    "            months = []\n",
    "            \n",
    "            for m in range(1,13):\n",
    "                S = gdal.Open(filebase + '_' + str(m) + '.vrt')\n",
    "                A = np.transpose(S.ReadAsArray(RX[i], RY[i], tilesize, tilesize))\n",
    "                if np.min(A) == 0.0:\n",
    "                    skip_loc = True\n",
    "                    break\n",
    "                else:\n",
    "                    months.append(A)                \n",
    "            if not skip_loc:\n",
    "                months = np.array(months)\n",
    "                samples.append(months)\n",
    "        \n",
    "    return np.array(samples)\n",
    "\n",
    "def normalization_parameters(fn):\n",
    "    from osgeo import gdal\n",
    "    S = gdal.Open(fn)\n",
    "    mns = []\n",
    "    sds = []\n",
    "    maxs = []\n",
    "    \n",
    "    for b in range(S.RasterCount):\n",
    "        B = S.GetRasterBand(b+1)\n",
    "        mn, sd = B.ComputeStatistics(1)[2:4]\n",
    "        mns.append(mn)\n",
    "        sds.append(sd)\n",
    "        maxs.append(B.GetMaximum())\n",
    "        \n",
    "    return mns, sds, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "    \n",
    "def fc_layer(inp, channels_in, channels_out, name='fc'):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.zeros([channels_in, channels_out]), name='W')\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "        return tf.nn.relu(tf.matmul(inp, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/run/media/ron/silver_small/twelve_months/3d/S1A_IW_GRDH_1SDV_20160325T083601_20160325T083630_010523_00FA23_6F51.tif'\n",
    "stats = normalization_parameters(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reconstructed_images:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(tf.float32, shape=([None, len_edge, len_edge, num_channels]))#bs\n",
    "    tf.summary.image('input_images', X, max_outputs=5)\n",
    "\n",
    "cur_input = X\n",
    "\n",
    "Ws = []    \n",
    "shapes = []\n",
    "\n",
    "for l, n_out in enumerate(n_filters[1:]):\n",
    "    n_input = cur_input.get_shape().as_list()[3]\n",
    "    shapes.append(cur_input.get_shape().as_list())\n",
    "    with tf.name_scope('conv_indecon_' + str(l)):\n",
    "        W = tf.Variable(tf.random_uniform([3, 3, n_input, n_out], -1.0/math.sqrt(n_input), 1.0/math.sqrt(n_input)), name='weights')\n",
    "        b = tf.Variable(tf.zeros([n_out]), name='bias')\n",
    "        tf.summary.histogram('weights', W)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        Ws.append(W)\n",
    "        conv = tf.nn.conv2d(cur_input, W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        act = tf.nn.sigmoid(conv+b)#tf.nn.relu(conv,b)\n",
    "        tf.summary.histogram('activations', act)\n",
    "    cur_input = act\n",
    "\n",
    "with tf.name_scope('Dense'):\n",
    "    with tf.name_scope('Fully_Encode'):\n",
    "        flattened = tf.reshape(cur_input, [-1, inedge * inedge * n_filters[-1]])# ...\n",
    "        \n",
    "        W_enc = weight_variable([inedge * inedge * n_filters[-1], h_dim], 'W_enc')\n",
    "        b_enc = bias_variable([h_dim], 'b_enc')        \n",
    "        full1 = tf.nn.sigmoid(tf.matmul(flattened, W_enc) + b_enc)\n",
    "        tf.summary.histogram('weights', W_enc)\n",
    "        tf.summary.histogram('biases', b_enc)\n",
    "        tf.summary.histogram('activations', full1)\n",
    "#        full1 = fc_layer(flattened, 7 * 7 * n_filters[-1], h_dim, 'fc1')\n",
    "    with tf.name_scope('Mu'):\n",
    "        W_mu = weight_variable([h_dim, latent_dim], 'W_mu')\n",
    "        b_mu = bias_variable([latent_dim], 'b_mu')\n",
    "        mu = tf.matmul(full1, W_mu) + b_mu\n",
    "    with tf.name_scope('Logstd'):\n",
    "        W_logstd = weight_variable([h_dim, latent_dim], 'W_logstd')\n",
    "        b_logstd = bias_variable([latent_dim], 'b_logstd')\n",
    "        logstd = tf.matmul(full1, W_logstd) + b_logstd\n",
    "    with tf.name_scope('VAE_final'):\n",
    "        noise = tf.random_normal([1, latent_dim])\n",
    "        z = mu + tf.multiply(noise, tf.exp(.5*logstd))\n",
    "    with tf.name_scope('Fully_Decode'):\n",
    "        \n",
    "        W_dec1 = weight_variable([latent_dim, h_dim], 'W_dec1')\n",
    "        b_dec1 = bias_variable([h_dim], 'b_dec1')\n",
    "        \n",
    "        full2 = tf.nn.sigmoid(tf.matmul(z, W_dec1) + b_dec1)\n",
    "        \n",
    "        tf.summary.histogram('weights_1', W_dec1)\n",
    "        tf.summary.histogram('biases_1', b_dec1)\n",
    "        tf.summary.histogram('activations_1', full2)\n",
    "        \n",
    "        W_dec2 = weight_variable([h_dim, inedge * inedge * n_filters[-1]], 'W_dec2')\n",
    "        b_dec2 = bias_variable([inedge * inedge * n_filters[-1]], 'b_dec2')\n",
    "        \n",
    "        full3 = tf.nn.sigmoid(tf.matmul(full2, W_dec2) + b_dec2)\n",
    "        \n",
    "        tf.summary.histogram('weights_2', W_dec2)\n",
    "        tf.summary.histogram('biases_2', b_dec2)\n",
    "        tf.summary.histogram('activations_2', full3)\n",
    "#        full2 = fc_layer(z, latent_dim, h_dim, 'fc2')\n",
    "#        full3 = fc_layer(full2, h_dim, 7 * 7 * n_filters[-1], 'fc3')\n",
    "        reshaped = tf.reshape(full3, [-1, inedge, inedge, n_filters[-1]])\n",
    "        \n",
    "z_visual = tf.reshape(z, [-1, latvisdim, latvisdim, 1])\n",
    "tf.summary.image('latents', z_visual, max_outputs=5)\n",
    "tf.summary.histogram('Latent', z)\n",
    "\n",
    "Ws.reverse()\n",
    "shapes.reverse()\n",
    "cur_input = reshaped\n",
    "\n",
    "for l, shape in enumerate(shapes):\n",
    "    cur_name = 'deconv' + str(l)  \n",
    "    W = Ws[l]    \n",
    "    with tf.name_scope('conv_indecon_' + str(len(Ws)-(l+1))):\n",
    "        b = tf.Variable(tf.zeros([W.get_shape().as_list()[2]]), name='bias_dec_'+str(l))\n",
    "        dec = tf.nn.conv2d_transpose(cur_input, W, tf.stack([bs, shape[1], shape[2], shape[3]]), strides=[1,2,2,1], padding='SAME')\n",
    "        if l+1 < len(shapes):\n",
    "            act = tf.nn.sigmoid(dec+b)\n",
    "            cur_input = act\n",
    "\n",
    "with tf.name_scope('reconst'):\n",
    "    reconstruction = tf.nn.sigmoid(dec + b)\n",
    "\n",
    "tf.summary.image('reconstructed_images', reconstruction, max_outputs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_flat = tf.contrib.layers.flatten(X)\n",
    "R_flat = tf.contrib.layers.flatten(reconstruction)\n",
    "\n",
    "log_likelihood = tf.reduce_sum(X_flat*tf.log(R_flat + 1e-9)+(1 - X_flat)*tf.log(1 - R_flat + 1e-9), reduction_indices=1)\n",
    "tf.summary.scalar('LogLike', tf.reduce_mean(log_likelihood))\n",
    "\n",
    "KL_term = -.5*tf.reduce_sum(1 + 2*logstd - tf.pow(mu,2) - tf.exp(2*logstd), reduction_indices=1)\n",
    "tf.summary.scalar('KL', tf.reduce_mean(KL_term))\n",
    "\n",
    "variational_lower_bound = tf.reduce_mean(log_likelihood - KL_term)\n",
    "tf.summary.scalar('cost', variational_lower_bound)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(-variational_lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter('./vae_logs/12')\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "400\n",
      "800\n",
      "1200\n",
      "1600\n",
      "2000\n",
      "2400\n",
      "2800\n",
      "3200\n",
      "3600\n",
      "4000\n",
      "4400\n",
      "4800\n",
      "5200\n",
      "5600\n",
      "6000\n",
      "6400\n",
      "6800\n",
      "7200\n",
      "7600\n",
      "8000\n",
      "8400\n",
      "8800\n",
      "9200\n",
      "9600\n",
      "10000\n",
      "10400\n",
      "10800\n",
      "11200\n",
      "11600\n",
      "12000\n",
      "12400\n",
      "12800\n",
      "13200\n",
      "13600\n",
      "14000\n",
      "14400\n",
      "14800\n",
      "15200\n",
      "15600\n",
      "16000\n",
      "16400\n",
      "16800\n",
      "17200\n",
      "17600\n",
      "18000\n",
      "18400\n",
      "18800\n",
      "19200\n",
      "19600\n",
      "20000\n",
      "20400\n",
      "20800\n",
      "21200\n",
      "21600\n",
      "22000\n",
      "22400\n",
      "22800\n",
      "23200\n",
      "23600\n",
      "24000\n",
      "24400\n",
      "24800\n",
      "25200\n",
      "25600\n",
      "26000\n",
      "26400\n",
      "26800\n",
      "27200\n",
      "27600\n",
      "28000\n",
      "28400\n",
      "28800\n",
      "29200\n",
      "29600\n",
      "30000\n",
      "30400\n",
      "30800\n",
      "31200\n",
      "31600\n",
      "32000\n",
      "32400\n",
      "32800\n",
      "33200\n",
      "33600\n",
      "34000\n",
      "34400\n",
      "34800\n",
      "35200\n",
      "35600\n",
      "36000\n",
      "36400\n",
      "36800\n",
      "37200\n",
      "37600\n",
      "38000\n",
      "38400\n",
      "38800\n",
      "39200\n",
      "39600\n",
      "40000\n",
      "40400\n",
      "40800\n",
      "41200\n",
      "41600\n",
      "42000\n",
      "42400\n",
      "42800\n",
      "43200\n",
      "43600\n",
      "44000\n",
      "44400\n",
      "44800\n",
      "45200\n",
      "45600\n",
      "46000\n",
      "46400\n",
      "46800\n",
      "47200\n",
      "47600\n",
      "48000\n",
      "48400\n",
      "48800\n",
      "49200\n",
      "49600\n",
      "50000\n",
      "50400\n",
      "50800\n",
      "51200\n",
      "51600\n",
      "52000\n",
      "52400\n",
      "52800\n",
      "53200\n",
      "53600\n",
      "54000\n",
      "54400\n",
      "54800\n",
      "55200\n",
      "55600\n",
      "56000\n",
      "56400\n",
      "56800\n",
      "57200\n",
      "57600\n",
      "58000\n",
      "58400\n",
      "58800\n",
      "59200\n",
      "59600\n",
      "60000\n",
      "60400\n",
      "60800\n",
      "61200\n",
      "61600\n",
      "62000\n",
      "62400\n",
      "62800\n",
      "63200\n",
      "63600\n",
      "64000\n",
      "64400\n",
      "64800\n",
      "65200\n",
      "65600\n",
      "66000\n",
      "66400\n",
      "66800\n",
      "67200\n",
      "67600\n",
      "68000\n",
      "68400\n",
      "68800\n",
      "69200\n",
      "69600\n",
      "70000\n",
      "70400\n",
      "70800\n",
      "71200\n",
      "71600\n",
      "72000\n",
      "72400\n",
      "72800\n",
      "73200\n",
      "73600\n",
      "74000\n",
      "74400\n",
      "74800\n",
      "75200\n",
      "75600\n",
      "76000\n",
      "76400\n",
      "76800\n",
      "77200\n",
      "77600\n",
      "78000\n",
      "78400\n",
      "78800\n",
      "79200\n",
      "79600\n",
      "80000\n",
      "80400\n",
      "80800\n",
      "81200\n",
      "81600\n",
      "82000\n",
      "82400\n",
      "82800\n",
      "83200\n",
      "83600\n",
      "84000\n",
      "84400\n",
      "84800\n",
      "85200\n",
      "85600\n",
      "86000\n",
      "86400\n",
      "86800\n",
      "87200\n",
      "87600\n",
      "88000\n",
      "88400\n",
      "88800\n",
      "89200\n",
      "89600\n",
      "90000\n",
      "90400\n",
      "90800\n",
      "91200\n",
      "91600\n",
      "92000\n",
      "92400\n",
      "92800\n",
      "93200\n",
      "93600\n",
      "94000\n",
      "94400\n",
      "94800\n",
      "95200\n",
      "95600\n",
      "96000\n",
      "96400\n",
      "96800\n",
      "97200\n",
      "97600\n",
      "98000\n",
      "98400\n",
      "98800\n",
      "99200\n",
      "99600\n",
      "100000\n",
      "100400\n",
      "100800\n",
      "101200\n",
      "101600\n",
      "102000\n",
      "102400\n",
      "102800\n",
      "103200\n",
      "103600\n",
      "104000\n",
      "104400\n",
      "104800\n",
      "105200\n",
      "105600\n",
      "106000\n",
      "106400\n",
      "106800\n",
      "107200\n",
      "107600\n",
      "108000\n",
      "108400\n",
      "108800\n",
      "109200\n",
      "109600\n",
      "110000\n",
      "110400\n",
      "110800\n",
      "111200\n",
      "111600\n",
      "112000\n",
      "112400\n",
      "112800\n",
      "113200\n",
      "113600\n",
      "114000\n",
      "114400\n",
      "114800\n",
      "115200\n",
      "115600\n",
      "116000\n",
      "116400\n",
      "116800\n",
      "117200\n",
      "117600\n",
      "118000\n",
      "118400\n",
      "118800\n",
      "119200\n",
      "119600\n",
      "120000\n",
      "120400\n",
      "120800\n",
      "121200\n",
      "121600\n",
      "122000\n",
      "122400\n",
      "122800\n",
      "123200\n",
      "123600\n",
      "124000\n",
      "124400\n",
      "124800\n",
      "125200\n",
      "125600\n",
      "126000\n",
      "126400\n",
      "126800\n",
      "127200\n",
      "127600\n",
      "128000\n",
      "128400\n",
      "128800\n",
      "129200\n",
      "129600\n",
      "130000\n",
      "130400\n",
      "130800\n",
      "131200\n",
      "131600\n",
      "132000\n",
      "132400\n",
      "132800\n",
      "133200\n",
      "133600\n",
      "134000\n",
      "134400\n",
      "134800\n",
      "135200\n",
      "135600\n",
      "136000\n",
      "136400\n",
      "136800\n",
      "137200\n",
      "137600\n",
      "138000\n",
      "138400\n",
      "138800\n",
      "139200\n",
      "139600\n",
      "140000\n",
      "140400\n",
      "140800\n",
      "141200\n",
      "141600\n",
      "142000\n",
      "142400\n",
      "142800\n",
      "143200\n",
      "143600\n",
      "144000\n",
      "144400\n",
      "144800\n",
      "145200\n",
      "145600\n",
      "146000\n",
      "146400\n",
      "146800\n",
      "147200\n",
      "147600\n",
      "148000\n",
      "148400\n",
      "148800\n",
      "149200\n",
      "149600\n",
      "150000\n",
      "150400\n",
      "150800\n",
      "151200\n",
      "151600\n",
      "152000\n",
      "152400\n",
      "152800\n",
      "153200\n",
      "153600\n",
      "154000\n",
      "154400\n",
      "154800\n",
      "155200\n",
      "155600\n",
      "156000\n",
      "156400\n",
      "156800\n",
      "157200\n",
      "157600\n",
      "158000\n",
      "158400\n",
      "158800\n",
      "159200\n",
      "159600\n",
      "160000\n",
      "160400\n",
      "160800\n",
      "161200\n",
      "161600\n",
      "162000\n",
      "162400\n",
      "162800\n",
      "163200\n",
      "163600\n",
      "164000\n",
      "164400\n",
      "164800\n",
      "165200\n",
      "165600\n",
      "166000\n",
      "166400\n",
      "166800\n",
      "167200\n",
      "167600\n",
      "168000\n",
      "168400\n",
      "168800\n",
      "169200\n",
      "169600\n",
      "170000\n",
      "170400\n",
      "170800\n",
      "171200\n",
      "171600\n",
      "172000\n",
      "172400\n",
      "172800\n",
      "173200\n",
      "173600\n",
      "174000\n",
      "174400\n",
      "174800\n",
      "175200\n",
      "175600\n",
      "176000\n",
      "176400\n",
      "176800\n",
      "177200\n",
      "177600\n",
      "178000\n",
      "178400\n",
      "178800\n",
      "179200\n",
      "179600\n",
      "180000\n",
      "180400\n",
      "180800\n",
      "181200\n",
      "181600\n",
      "182000\n",
      "182400\n",
      "182800\n",
      "183200\n",
      "183600\n",
      "184000\n",
      "184400\n",
      "184800\n",
      "185200\n",
      "185600\n",
      "186000\n",
      "186400\n",
      "186800\n",
      "187200\n",
      "187600\n",
      "188000\n",
      "188400\n",
      "188800\n",
      "189200\n",
      "189600\n",
      "190000\n",
      "190400\n",
      "190800\n",
      "191200\n",
      "191600\n",
      "192000\n",
      "192400\n",
      "192800\n",
      "193200\n",
      "193600\n",
      "194000\n",
      "194400\n",
      "194800\n",
      "195200\n",
      "195600\n",
      "196000\n",
      "196400\n",
      "196800\n",
      "197200\n",
      "197600\n",
      "198000\n",
      "198400\n",
      "198800\n",
      "199200\n",
      "199600\n",
      "200000\n",
      "200400\n",
      "200800\n",
      "201200\n",
      "201600\n",
      "202000\n",
      "202400\n",
      "202800\n",
      "203200\n",
      "203600\n",
      "204000\n",
      "204400\n",
      "204800\n",
      "205200\n",
      "205600\n",
      "206000\n",
      "206400\n",
      "206800\n",
      "207200\n",
      "207600\n",
      "208000\n",
      "208400\n",
      "208800\n",
      "209200\n",
      "209600\n",
      "210000\n",
      "210400\n",
      "210800\n",
      "211200\n",
      "211600\n",
      "212000\n",
      "212400\n",
      "212800\n",
      "213200\n",
      "213600\n",
      "214000\n",
      "214400\n",
      "214800\n",
      "215200\n",
      "215600\n",
      "216000\n",
      "216400\n",
      "216800\n",
      "217200\n",
      "217600\n",
      "218000\n",
      "218400\n",
      "218800\n",
      "219200\n",
      "219600\n",
      "220000\n",
      "220400\n",
      "220800\n",
      "221200\n",
      "221600\n",
      "222000\n",
      "222400\n",
      "222800\n",
      "223200\n",
      "223600\n",
      "224000\n",
      "224400\n",
      "224800\n",
      "225200\n",
      "225600\n",
      "226000\n",
      "226400\n",
      "226800\n",
      "227200\n",
      "227600\n",
      "228000\n",
      "228400\n",
      "228800\n",
      "229200\n",
      "229600\n",
      "230000\n",
      "230400\n",
      "230800\n",
      "231200\n",
      "231600\n",
      "232000\n",
      "232400\n",
      "232800\n",
      "233200\n",
      "233600\n",
      "234000\n",
      "234400\n",
      "234800\n",
      "235200\n",
      "235600\n",
      "236000\n",
      "236400\n",
      "236800\n",
      "237200\n",
      "237600\n",
      "238000\n",
      "238400\n",
      "238800\n",
      "239200\n",
      "239600\n",
      "240000\n",
      "240400\n",
      "240800\n",
      "241200\n",
      "241600\n",
      "242000\n",
      "242400\n",
      "242800\n",
      "243200\n",
      "243600\n",
      "244000\n",
      "244400\n",
      "244800\n",
      "245200\n",
      "245600\n",
      "246000\n",
      "246400\n",
      "246800\n",
      "247200\n",
      "247600\n",
      "248000\n",
      "248400\n",
      "248800\n",
      "249200\n",
      "249600\n",
      "250000\n",
      "250400\n",
      "250800\n",
      "251200\n",
      "251600\n",
      "252000\n",
      "252400\n",
      "252800\n",
      "253200\n",
      "253600\n",
      "254000\n",
      "254400\n",
      "254800\n",
      "255200\n",
      "255600\n",
      "256000\n",
      "256400\n",
      "256800\n",
      "257200\n",
      "257600\n",
      "258000\n",
      "258400\n",
      "258800\n",
      "259200\n",
      "259600\n",
      "260000\n",
      "260400\n",
      "260800\n",
      "261200\n",
      "261600\n",
      "262000\n",
      "262400\n",
      "262800\n",
      "263200\n",
      "263600\n",
      "264000\n",
      "264400\n",
      "264800\n",
      "265200\n",
      "265600\n",
      "266000\n",
      "266400\n",
      "266800\n",
      "267200\n",
      "267600\n",
      "268000\n",
      "268400\n",
      "268800\n",
      "269200\n",
      "269600\n",
      "270000\n",
      "270400\n",
      "270800\n",
      "271200\n",
      "271600\n",
      "272000\n",
      "272400\n",
      "272800\n",
      "273200\n",
      "273600\n",
      "274000\n",
      "274400\n",
      "274800\n",
      "275200\n",
      "275600\n",
      "276000\n",
      "276400\n",
      "276800\n",
      "277200\n",
      "277600\n",
      "278000\n",
      "278400\n",
      "278800\n",
      "279200\n",
      "279600\n",
      "280000\n",
      "280400\n",
      "280800\n",
      "281200\n",
      "281600\n",
      "282000\n",
      "282400\n",
      "282800\n",
      "283200\n",
      "283600\n",
      "284000\n",
      "284400\n",
      "284800\n",
      "285200\n",
      "285600\n",
      "286000\n",
      "286400\n",
      "286800\n",
      "287200\n",
      "287600\n",
      "288000\n",
      "288400\n",
      "288800\n",
      "289200\n",
      "289600\n",
      "290000\n",
      "290400\n",
      "290800\n",
      "291200\n",
      "291600\n",
      "292000\n",
      "292400\n",
      "292800\n",
      "293200\n",
      "293600\n",
      "294000\n",
      "294400\n",
      "294800\n",
      "295200\n",
      "295600\n",
      "296000\n",
      "296400\n",
      "296800\n",
      "297200\n",
      "297600\n",
      "298000\n",
      "298400\n",
      "298800\n",
      "299200\n",
      "299600\n",
      "300000\n",
      "300400\n",
      "300800\n",
      "301200\n",
      "301600\n",
      "302000\n",
      "302400\n",
      "302800\n",
      "303200\n",
      "303600\n",
      "304000\n",
      "304400\n",
      "304800\n",
      "305200\n",
      "305600\n",
      "306000\n",
      "306400\n",
      "306800\n",
      "307200\n",
      "307600\n",
      "308000\n",
      "308400\n",
      "308800\n",
      "309200\n",
      "309600\n",
      "310000\n",
      "310400\n",
      "310800\n",
      "311200\n",
      "311600\n",
      "312000\n",
      "312400\n",
      "312800\n",
      "313200\n",
      "313600\n",
      "314000\n",
      "314400\n",
      "314800\n",
      "315200\n",
      "315600\n",
      "316000\n",
      "316400\n",
      "316800\n",
      "317200\n",
      "317600\n",
      "318000\n",
      "318400\n",
      "318800\n",
      "319200\n",
      "319600\n",
      "320000\n",
      "320400\n",
      "320800\n",
      "321200\n",
      "321600\n",
      "322000\n",
      "322400\n",
      "322800\n",
      "323200\n",
      "323600\n",
      "324000\n",
      "324400\n",
      "324800\n",
      "325200\n",
      "325600\n",
      "326000\n",
      "326400\n",
      "326800\n",
      "327200\n",
      "327600\n",
      "328000\n",
      "328400\n",
      "328800\n",
      "329200\n",
      "329600\n",
      "330000\n",
      "330400\n",
      "330800\n",
      "331200\n",
      "331600\n",
      "332000\n",
      "332400\n",
      "332800\n",
      "333200\n",
      "333600\n",
      "334000\n",
      "334400\n",
      "334800\n",
      "335200\n",
      "335600\n",
      "336000\n",
      "336400\n",
      "336800\n",
      "337200\n",
      "337600\n",
      "338000\n",
      "338400\n",
      "338800\n",
      "339200\n",
      "339600\n",
      "340000\n",
      "340400\n",
      "340800\n",
      "341200\n",
      "341600\n",
      "342000\n",
      "342400\n",
      "342800\n",
      "343200\n",
      "343600\n",
      "344000\n",
      "344400\n",
      "344800\n",
      "345200\n",
      "345600\n",
      "346000\n",
      "346400\n",
      "346800\n",
      "347200\n",
      "347600\n",
      "348000\n",
      "348400\n",
      "348800\n",
      "349200\n",
      "349600\n",
      "350000\n",
      "350400\n",
      "350800\n",
      "351200\n",
      "351600\n",
      "352000\n",
      "352400\n",
      "352800\n",
      "353200\n",
      "353600\n",
      "354000\n",
      "354400\n",
      "354800\n",
      "355200\n",
      "355600\n",
      "356000\n",
      "356400\n",
      "356800\n",
      "357200\n",
      "357600\n",
      "358000\n",
      "358400\n",
      "358800\n",
      "359200\n",
      "359600\n",
      "360000\n",
      "360400\n",
      "360800\n",
      "361200\n",
      "361600\n",
      "362000\n",
      "362400\n",
      "362800\n",
      "363200\n",
      "363600\n",
      "364000\n",
      "364400\n",
      "364800\n",
      "365200\n",
      "365600\n",
      "366000\n",
      "366400\n",
      "366800\n",
      "367200\n",
      "367600\n",
      "368000\n",
      "368400\n",
      "368800\n",
      "369200\n",
      "369600\n",
      "370000\n",
      "370400\n",
      "370800\n",
      "371200\n",
      "371600\n",
      "372000\n",
      "372400\n",
      "372800\n",
      "373200\n",
      "373600\n",
      "374000\n",
      "374400\n",
      "374800\n",
      "375200\n",
      "375600\n",
      "376000\n",
      "376400\n",
      "376800\n",
      "377200\n",
      "377600\n",
      "378000\n",
      "378400\n",
      "378800\n",
      "379200\n",
      "379600\n",
      "380000\n",
      "380400\n",
      "380800\n",
      "381200\n",
      "381600\n",
      "382000\n",
      "382400\n",
      "382800\n",
      "383200\n",
      "383600\n",
      "384000\n",
      "384400\n",
      "384800\n",
      "385200\n",
      "385600\n",
      "386000\n",
      "386400\n",
      "386800\n",
      "387200\n",
      "387600\n",
      "388000\n",
      "388400\n",
      "388800\n",
      "389200\n",
      "389600\n",
      "390000\n",
      "390400\n",
      "390800\n",
      "391200\n",
      "391600\n",
      "392000\n",
      "392400\n",
      "392800\n",
      "393200\n",
      "393600\n",
      "394000\n",
      "394400\n",
      "394800\n",
      "395200\n",
      "395600\n",
      "396000\n",
      "396400\n",
      "396800\n",
      "397200\n",
      "397600\n",
      "398000\n",
      "398400\n",
      "398800\n",
      "399200\n",
      "399600\n",
      "400000\n",
      "400400\n",
      "400800\n",
      "401200\n",
      "401600\n",
      "402000\n",
      "402400\n",
      "402800\n",
      "403200\n",
      "403600\n",
      "404000\n",
      "404400\n",
      "404800\n",
      "405200\n",
      "405600\n",
      "406000\n",
      "406400\n",
      "406800\n",
      "407200\n",
      "407600\n",
      "408000\n",
      "408400\n",
      "408800\n",
      "409200\n",
      "409600\n",
      "410000\n",
      "410400\n",
      "410800\n",
      "411200\n",
      "411600\n",
      "412000\n",
      "412400\n",
      "412800\n",
      "413200\n",
      "413600\n",
      "414000\n",
      "414400\n",
      "414800\n",
      "415200\n",
      "415600\n",
      "416000\n",
      "416400\n",
      "416800\n",
      "417200\n",
      "417600\n",
      "418000\n",
      "418400\n",
      "418800\n",
      "419200\n",
      "419600\n",
      "420000\n",
      "420400\n",
      "420800\n",
      "421200\n",
      "421600\n",
      "422000\n",
      "422400\n",
      "422800\n",
      "423200\n",
      "423600\n",
      "424000\n",
      "424400\n",
      "424800\n",
      "425200\n",
      "425600\n",
      "426000\n",
      "426400\n",
      "426800\n",
      "427200\n",
      "427600\n",
      "428000\n",
      "428400\n",
      "428800\n",
      "429200\n",
      "429600\n",
      "430000\n",
      "430400\n",
      "430800\n",
      "431200\n",
      "431600\n",
      "432000\n",
      "432400\n",
      "432800\n",
      "433200\n",
      "433600\n",
      "434000\n",
      "434400\n",
      "434800\n",
      "435200\n",
      "435600\n",
      "436000\n",
      "436400\n",
      "436800\n",
      "437200\n",
      "437600\n",
      "438000\n",
      "438400\n",
      "438800\n",
      "439200\n",
      "439600\n",
      "440000\n",
      "440400\n",
      "440800\n",
      "441200\n",
      "441600\n",
      "442000\n",
      "442400\n",
      "442800\n",
      "443200\n",
      "443600\n",
      "444000\n",
      "444400\n",
      "444800\n",
      "445200\n",
      "445600\n",
      "446000\n",
      "446400\n",
      "446800\n",
      "447200\n",
      "447600\n",
      "448000\n",
      "448400\n",
      "448800\n",
      "449200\n",
      "449600\n",
      "450000\n",
      "450400\n",
      "450800\n",
      "451200\n",
      "451600\n",
      "452000\n",
      "452400\n",
      "452800\n",
      "453200\n",
      "453600\n",
      "454000\n",
      "454400\n",
      "454800\n",
      "455200\n",
      "455600\n",
      "456000\n",
      "456400\n",
      "456800\n",
      "457200\n",
      "457600\n",
      "458000\n",
      "458400\n",
      "458800\n",
      "459200\n",
      "459600\n",
      "460000\n",
      "460400\n",
      "460800\n",
      "461200\n",
      "461600\n",
      "462000\n",
      "462400\n",
      "462800\n",
      "463200\n",
      "463600\n",
      "464000\n",
      "464400\n",
      "464800\n",
      "465200\n",
      "465600\n",
      "466000\n",
      "466400\n",
      "466800\n",
      "467200\n",
      "467600\n",
      "468000\n",
      "468400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0155fc599a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#    x_batch = sample_a_batch(filename, bs, len_edge, sb=2, normalize=stats, flattened=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mrecording_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ron/.conda/envs/tensorflow_latest/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 1000000\n",
    "recording_interval = 400\n",
    "variational_lower_bound_array = []\n",
    "log_likelihood_array = []\n",
    "KL_term_array = []\n",
    "iteration_array = [i*recording_interval for i in range(num_iterations/recording_interval)]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    mn_l = mnist.train.next_batch(bs)[0]\n",
    "    x_batch = np.reshape(mn_l, [bs, len_edge, len_edge, 1]) # for mnist\n",
    "#    x_batch = sample_a_batch(filename, bs, len_edge, sb=2, normalize=stats, flattened=False)\n",
    "    _, s = sess.run([optimizer, merged_summary], feed_dict={X: x_batch})    \n",
    "    if (i%recording_interval == 0):\n",
    "        print i\n",
    "        writer.add_summary(s, i)\n",
    "        #every 1K iterations record these values\n",
    "#        vlb_eval = variational_lower_bound.eval(feed_dict={X: x_batch})\n",
    "#        print \"Iteration: {}, Loss: {}\".format(i, vlb_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IGNORE FOLLOWING CELLS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_layer(inp, channels_in, channels_out, vscope, name='conv'):\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scopec(vscope):\n",
    "            tf.Variable(tf.zeros([3, 3, channels_in, channels_out]), name='W')\n",
    "        b = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "        conv = tf.nn.conv2d(inp, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('activations', act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "def conv_t_layer(inp, channels_in, channels_out, name='conv_t'):#, activation=tf.nn.relu):\n",
    "    with tf.name_scope(name):\n",
    "        #w = tf.Variable(tf.zeros([3, 3, channels_out, channels_in]), name='W')\n",
    "        w = conv1\n",
    "        b = tf.Variable(tf.zeros([conv1.get_shape().as_list()[2]]))\n",
    "        \n",
    "        tf.nn.conv2d_transpose(inp, W, tf.stack([]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        b = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "        batch_size = 32 #tf.shape(inp)[0]\n",
    "        deconv_shape = tf.stack([batch_size, inp.shape[2].value * 2, inp.shape[2].value * 2, channels_out])\n",
    "        conv_t = tf.nn.conv2d_transpose(inp, w, deconv_shape, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #act = activation(conv_t + b)\n",
    "        act = tf.nn.relu(conv_t + b)\n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('activations', act)\n",
    "        return act#tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(tf.float32, shape=([None, len_edge, len_edge, num_channels]))#bs\n",
    "    tf.summary.image('input_images', X, max_outputs=3)\n",
    "\n",
    "conv1 = conv_layer(X, num_channels, filters[0], 'conv1')\n",
    "conv2 = conv_layer(conv1, conv1.shape[-1].value, filters[1], 'conv2')\n",
    "conv3 = conv_layer(conv2, conv2.shape[-1].value, filters[2], 'conv3')\n",
    "conv4 = conv_layer(conv3, conv3.shape[-1].value, filters[3], 'conv4')\n",
    "\n",
    "#with tf.name_scope('Dense_Encode'):\n",
    "flattened = tf.reshape(conv4, [-1, 2 * 2 * filters[-1]])\n",
    "full1 = fc_layer(flattened, 2 * 2 * filters[-1], h_dim, 'fc1') # channels_in ?????\n",
    "\n",
    "W_mu = weight_variable([h_dim, latent_dim], 'W_mu')\n",
    "b_mu = bias_variable([latent_dim], 'b_mu')\n",
    "mu = FC_layer(full1, W_mu, b_mu)\n",
    "W_logstd = weight_variable([h_dim, latent_dim], 'W_logstd')\n",
    "b_logstd = bias_variable([latent_dim], 'b_logstd')\n",
    "logstd = FC_layer(full1, W_logstd, b_logstd)\n",
    "\n",
    "noise = tf.random_normal([1, latent_dim])\n",
    "z = mu + tf.multiply(noise, tf.exp(.5*logstd))\n",
    "\n",
    "z_visual = tf.reshape(z, [-1, 6, 6, 1])\n",
    "tf.summary.image('latents', z_visual, max_outputs=3)\n",
    "\n",
    "tf.summary.histogram('Latent', z)\n",
    "\n",
    "full2 = fc_layer(z, latent_dim, h_dim, 'fc2')\n",
    "full3 = fc_layer(full2, h_dim, 2 * 2 * filters[-1], 'fc3')# ???????\n",
    "\n",
    "reshaped = tf.reshape(full3, [-1, 2, 2, filters[-1]])\n",
    "\n",
    "conv_t1 = conv_t_layer(reshaped, filters[1], filters[1], 'conv_t1')\n",
    "conv_t2 = conv_t_layer(conv_t1, filters[1], filters[1], 'conv_t2')\n",
    "conv_t3 = conv_t_layer(conv_t2, filters[1], filters[1], 'conv_t3')\n",
    "reconstruction = conv_t_layer(conv_t3, filters[1], 1, 'conv_t4')#, tf.nn.sigmoid)\n",
    "\n",
    "tf.summary.image('reconstructed_images', reconstruction, max_outputs=3) #............"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
